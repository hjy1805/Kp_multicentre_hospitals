{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85af3e1-877e-4712-9d88-1ecfd8992af7",
   "metadata": {},
   "source": [
    "# ðŸ§  Machine Learning Pipeline for Clinical Outcome Prediction\n",
    "\n",
    "This notebook implements the full machine-learning pipeline used in our study to predict three clinical outcomes:\n",
    "\n",
    "- **In-hospital Mortality**\n",
    "- **ICU Admission**\n",
    "- **Length of Stay (LOS)**\n",
    "\n",
    "It is organised into clearly defined stages covering data inputs, cross-validation setup, feature set construction, model training, and output generation.\n",
    "\n",
    "---\n",
    "\n",
    "##  Input Files (Human-Readable Description)\n",
    "\n",
    "The pipeline uses the following data matrices and corresponding feature name lists:\n",
    "\n",
    "| Variable         | Description |\n",
    "|------------------|-------------|\n",
    "| `X_clin`         | Clinical variables (e.g., age, vitals, comorbidities) |\n",
    "| `clinical_names` | Feature names for clinical variables |\n",
    "| `X_amrvir`       | Antimicrobial resistance & virulence gene features |\n",
    "| `amrvir_names`   | AMR/Virulence feature names |\n",
    "| `X_pangenome`    | Pangenome presence/absence features |\n",
    "| `pangenome_names`| Pangenome feature names |\n",
    "| `X_snps`         | Core genome SNP features |\n",
    "| `snps_names`     | SNP feature names |\n",
    "| `X_unitigs`      | Unitig sequence features |\n",
    "| `unitigs_names`  | Unitig feature names |\n",
    "\n",
    "---\n",
    "\n",
    "## PHASE A Creating Cross-Validation Folds\n",
    "\n",
    "We generate **five outer cross-validation folds** of train, validation, and test splits.\n",
    "\n",
    "- The indices for each fold are saved and reused across all models to ensure consistent comparison.\n",
    "- Each fold is used for model training, selection, and held-out evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## PHASE B Feature Set Configurations\n",
    "\n",
    "Different combinations of data layers are defined via **postfix keys**.  \n",
    "To switch the prediction target, replace the postfix suffix `Mortality` with:\n",
    "\n",
    "- `ICU` â†’ ICU Admission  \n",
    "- `LOS` â†’ Length of Stay\n",
    "\n",
    "```python\n",
    "postfix_data = {\n",
    "    \"_1_Mortality\": ([X_clin], clinical_names),\n",
    "\n",
    "    \"_2_Mortality\": ([X_clin, X_amrvir],\n",
    "                     clinical_names + amrvir_names),\n",
    "\n",
    "    \"_3_Mortality\": ([X_clin, X_amrvir, X_pangenome],\n",
    "                     clinical_names + amrvir_names + pangenome_names),\n",
    "\n",
    "    \"_4_Mortality\": ([X_clin, X_amrvir, X_pangenome, X_snps],\n",
    "                     clinical_names + amrvir_names + pangenome_names + snps_names),\n",
    "\n",
    "    \"_5_Mortality\": ([X_clin, X_amrvir, X_pangenome, X_snps, X_unitigs],\n",
    "                     clinical_names + amrvir_names + pangenome_names + snps_names + unitigs_names),\n",
    "\n",
    "    \"_6_Mortality\": ([X_amrvir, X_pangenome, X_snps, X_unitigs],\n",
    "                     amrvir_names + pangenome_names + snps_names + unitigs_names)\n",
    "}\n",
    "```\n",
    "\n",
    "## PHASE C Machine Learning Training Overview\n",
    "\n",
    "This section details which algorithms are trained for each outcome type.\n",
    "\n",
    "---\n",
    "\n",
    "#### Binary Outcomes: Mortality & ICU Admission\n",
    "\n",
    "For Mortality and ICU Admission prediction, the following classifiers are trained:\n",
    "\n",
    "â€¢ **Elastic Net Logistic Regression**  \n",
    "Trained on each fold and feature set; balances L1 and L2 penalties for feature selection and regularisation.\n",
    "\n",
    "â€¢ **XGBoost Classifier**  \n",
    "Trained on each fold and feature set; gradient-boosted trees that model non-linear relationships.  \n",
    "Feature importance analysis is also provided for XGBoost using metrics such as gain and/or SHAP values.\n",
    "\n",
    "âœ” Both Elastic Net and XGBoost classifiers are trained for Mortality and ICU tasks.  \n",
    "âœ” XGBoost training includes downstream feature importance analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### Continuous Outcome: Length of Stay (LOS)\n",
    "\n",
    "For LOS regression, the following models are trained:\n",
    "\n",
    "â€¢ **Quantile Regressor**  \n",
    "Estimates conditional quantiles of LOS (e.g., median and tails) to capture distributional effects.\n",
    "\n",
    "â€¢ **NGBoost**  \n",
    "A probabilistic gradient boosting model that produces predictive distributions and uncertainty estimates.\n",
    "\n",
    "âœ” Both Quantile Regression and NGBoost are trained for LOS prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3147c8-5f41-476b-bc8c-49d7782310fd",
   "metadata": {},
   "source": [
    "## PHASE A â€” Save Train/Test Indices\n",
    "\n",
    "The following script generates 5 non-stratified folds for a continuous outcome using `KFold` from scikit-learn. For each outer fold, a random 15 % subset of the training indices is also held out for internal validation. All fold indices along with feature names are saved as compressed `.npz` files. A splits manifest JSON records split parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc53cc7-2abb-495d-a034-ce930e6eddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# =========================\n",
    "# SAVE NON-OVERLAPPING CV SPLITS (classification)\n",
    "# =========================\n",
    "# Required in memory:\n",
    "#   X : np.ndarray (n_samples, n_features)\n",
    "#   y : np.ndarray (n_samples,)   # binary or categorical target\n",
    "#   feature_names : list[str]\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Project root (portable)\n",
    "# ------------------------------------------------------\n",
    "BASE_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "\n",
    "folds_dir = BASE_DIR / \"saved_folds_Mortality\"\n",
    "folds_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Parameters\n",
    "# ------------------------------------------------------\n",
    "N_FOLDS      = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Create and save fold indices\n",
    "# ------------------------------------------------------\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):\n",
    "\n",
    "    np.savez_compressed(\n",
    "        folds_dir / f\"fold{fold}_idx.npz\",\n",
    "        train_idx=train_idx,\n",
    "        test_idx=test_idx,\n",
    "        feature_names=np.asarray(feature_names, dtype=object)\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Manifest file\n",
    "# ------------------------------------------------------\n",
    "with open(folds_dir / \"splits_manifest.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"n_samples\"      : int(X.shape[0]),\n",
    "        \"n_features\"     : int(X.shape[1]),\n",
    "        \"n_folds\"        : N_FOLDS,\n",
    "        \"target_type\"    : \"classification\",\n",
    "        \"stratification\" : True,\n",
    "        \"validation_set\" : False\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Saved {N_FOLDS} non-overlapping folds in: {folds_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63212d29-fe5e-4607-9ae2-0bcce7eb7746",
   "metadata": {},
   "source": [
    "## PHASE B â€” Create input dataframe for the predictive classifiers \n",
    "\n",
    "The following script generates input predictive data and label data for the the two nominal labels, i.e. **Moratlity** (Death) and **ICU Admission**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff797a-12b8-4276-8ead-55e7c97f92d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "BASE_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "DATA_DIR = BASE_DIR\n",
    "postfix = \"_3_Mortality\" # or _2_Morality based on the desired input\n",
    "# ------------------------\n",
    "# Load datasets\n",
    "# ------------------------\n",
    "df_X = pd.read_csv(DATA_DIR / \"Metadata_predictor_Mortality.csv\")\n",
    "df_amrvir = pd.read_csv(DATA_DIR / \"AMRVIR_predictor_Mortality.csv\")\n",
    "df_pangenome = pd.read_csv(DATA_DIR / \"Pangenome_predictor_Mortality.csv\")\n",
    "df_snps = pd.read_csv(DATA_DIR / \"SNPs_predictor_Mortality.csv\")\n",
    "df_unitigs = pd.read_csv(DATA_DIR / \"Unitigs_predictor_Mortality.csv\")\n",
    "df_pheno = pd.read_csv(DATA_DIR / \"df_phenotype_Mortality.csv\")\n",
    "\n",
    "# ------------------------\n",
    "# Merge once (perfect alignment)\n",
    "# ------------------------\n",
    "df = (\n",
    "    df_X\n",
    "    .merge(df_amrvir, on=ID_COL)\n",
    "    .merge(df_pangenome, on=ID_COL)\n",
    "    .merge(df_snps, on=ID_COL)\n",
    "    .merge(df_unitigs, on=ID_COL)\n",
    "    .merge(df_pheno[[ID_COL, OUTCOME_COL]], on=ID_COL)\n",
    ")\n",
    "\n",
    "y = df[OUTCOME_COL].astype(int).to_numpy()\n",
    "\n",
    "# ------------------------\n",
    "# Clinical preprocessing\n",
    "# ------------------------\n",
    "feature_cols_num = [\"CHARLSON\",\"AGE\",\"BMI\"]\n",
    "feature_cols_cat = [\"GENDER\",\"SOURCE\",\"RGN\"]\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imp\",SimpleImputer(strategy=\"median\")),\n",
    "                      (\"sc\",StandardScaler())]), feature_cols_num),\n",
    "    (\"cat\", Pipeline([(\"imp\",SimpleImputer(strategy=\"most_frequent\")),\n",
    "                      (\"ohe\",OneHotEncoder(handle_unknown=\"ignore\"))]), feature_cols_cat)\n",
    "])\n",
    "\n",
    "X_clin = preprocess.fit_transform(df[feature_cols_num + feature_cols_cat])\n",
    "cat_names = preprocess.named_transformers_[\"cat\"].named_steps[\"ohe\"].get_feature_names_out(feature_cols_cat)\n",
    "clinical_names = list(feature_cols_num) + list(cat_names)\n",
    "clinical_names = [f\"{c}_clinical\" for c in clinical_names]\n",
    "\n",
    "# ------------------------\n",
    "# Omics blocks\n",
    "# ------------------------\n",
    "def block(df_source, suffix):\n",
    "    cols = [c for c in df_source.columns if c != ID_COL]\n",
    "    return df[cols].to_numpy(), [f\"{c}_{suffix}\" for c in cols]\n",
    "\n",
    "X_amrvir, amrvir_names = block(df_amrvir, \"amrvir\")\n",
    "X_pangenome, pangenome_names = block(df_pangenome, \"pangenome\")\n",
    "X_snps, snps_names = block(df_snps, \"snps\")\n",
    "X_unitigs, unitigs_names = block(df_unitigs, \"unitigs\")\n",
    "\n",
    "# ------------------------\n",
    "# Postfix â†’ feature map\n",
    "# ------------------------\n",
    "FEATURE_MAP = {\n",
    "    \"_1_Mortality\": ([X_clin], clinical_names),\n",
    "    \"_2_Mortality\": ([X_clin, X_amrvir], clinical_names + amrvir_names),\n",
    "    \"_3_Mortality\": ([X_clin, X_amrvir, X_pangenome],\n",
    "                     clinical_names + amrvir_names + pangenome_names),\n",
    "    \"_4_Mortality\": ([X_clin, X_amrvir, X_pangenome, X_snps],\n",
    "                     clinical_names + amrvir_names + pangenome_names + snps_names),\n",
    "    \"_5_Mortality\": ([X_clin, X_amrvir, X_pangenome, X_snps, X_unitigs],\n",
    "                     clinical_names + amrvir_names + pangenome_names + snps_names + unitigs_names),\n",
    "    \"_6_Mortality\": ([X_amrvir, X_pangenome, X_snps, X_unitigs]\n",
    "}\n",
    "\n",
    "# ------------------------\n",
    "# Build X\n",
    "# ------------------------\n",
    "blocks, feature_names = FEATURE_MAP[postfix]\n",
    "X = np.hstack(blocks).astype(np.float32)\n",
    "\n",
    "df = pd.read_csv(DATA_DIR / \"df_phenotype_Mortality.csv\")\n",
    "# Assign labels (y) from the Death column\n",
    "y = df[\"Mortality\"].values\n",
    "\n",
    "#For LOS make this log1p transformation beforehand\n",
    "y =  y.astype(np.int32)       \n",
    "y =  np.log1p(y)\n",
    "\n",
    "print(\"postfix:\", postfix)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e893d4a8-278e-4f03-aafc-f24b91df0e60",
   "metadata": {},
   "source": [
    "# PHASE C /A/Classifers - Elastic-Net Logistic Regression with nested CV\n",
    "\n",
    "- Expects X, y, feature_names in memory\n",
    "- Uses saved train/test indices: saved_folds/fold{n}_idx.npz\n",
    "- Performs 3-fold inner CV for hyperparameter tuning\n",
    "- Saves models, metrics (train/val/test), and OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc21400-16e3-4977-bd14-85cc452be317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# --- Config ---\n",
    "base        = Path(\"saved_folds\").parent\n",
    "folds_dir   = base / \"saved_folds\"\n",
    "models_dir  = base / f\"saved_models_lr{postfix}\"\n",
    "preds_dir   = base / f\"preds_lr{postfix}\"\n",
    "metrics_dir = base / f\"metrics_lr{postfix}\"\n",
    "inner_val_dir = metrics_dir / \"inner_val\"\n",
    "\n",
    "for d in [models_dir, preds_dir, metrics_dir, inner_val_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "n_splits_outer = 5\n",
    "n_splits_inner = 3\n",
    "\n",
    "grid_lr = {\n",
    "    \"C\": [0.1, 1.0],\n",
    "    \"l1_ratio\": [0.0, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "def make_metrics(y_true, y_prob, thr=0.5):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    return {\n",
    "        \"auc\":  roc_auc_score(y_true, y_prob),\n",
    "        \"acc\":  accuracy_score(y_true, y_hat),\n",
    "        \"prec\": precision_score(y_true, y_hat, zero_division=0),\n",
    "        \"rec\":  recall_score(y_true, y_hat, zero_division=0),\n",
    "        \"f1\":   f1_score(y_true, y_hat, zero_division=0),\n",
    "    }\n",
    "\n",
    "# --- Nested CV ---\n",
    "all_rows = []\n",
    "oof_preds = np.zeros(len(y))\n",
    "oof_true  = np.zeros(len(y), dtype=int)\n",
    "\n",
    "for fold in range(1, n_splits_outer + 1):\n",
    "    idx = np.load(folds_dir / f\"fold{fold}_idx.npz\", allow_pickle=True)\n",
    "    train_idx, test_idx = idx[\"train_idx\"], idx[\"test_idx\"]\n",
    "\n",
    "    X_tr, y_tr = X[train_idx], y[train_idx]\n",
    "    X_te, y_te = X[test_idx], y[test_idx]\n",
    "\n",
    "    inner_cv = StratifiedKFold(n_splits=n_splits_inner, shuffle=True, random_state=42)\n",
    "    best_auc, best_params = -np.inf, None\n",
    "\n",
    "    for C, l1r in itertools.product(grid_lr[\"C\"], grid_lr[\"l1_ratio\"]):\n",
    "        aucs_inner, val_metrics = [], []\n",
    "\n",
    "        for inner_i, (it, iv) in enumerate(inner_cv.split(X_tr, y_tr), start=1):\n",
    "            sub_idx = train_idx[it]\n",
    "            val_idx = train_idx[iv]\n",
    "\n",
    "            pipe = Pipeline([\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"lr\", LogisticRegression(\n",
    "                    max_iter=5000,\n",
    "                    solver=\"saga\",\n",
    "                    penalty=\"elasticnet\",\n",
    "                    C=C,\n",
    "                    l1_ratio=l1r,\n",
    "                    random_state=42\n",
    "                ))\n",
    "            ])\n",
    "            pipe.fit(X[sub_idx], y[sub_idx])\n",
    "            val_probs = pipe.predict_proba(X[val_idx])[:, 1]\n",
    "\n",
    "            m = make_metrics(y[val_idx], val_probs)\n",
    "            m.update({\"outer_fold\": fold, \"inner_fold\": inner_i, \"C\": C, \"l1_ratio\": l1r})\n",
    "            val_metrics.append(m)\n",
    "            aucs_inner.append(m[\"auc\"])\n",
    "\n",
    "        pd.DataFrame(val_metrics).to_csv(\n",
    "            inner_val_dir / f\"lr_inner_val_fold{fold}_C{C}_l1{l1r}{postfix}.csv\",\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        mean_auc = np.mean(aucs_inner)\n",
    "        if mean_auc > best_auc:\n",
    "            best_auc = mean_auc\n",
    "            best_params = {\"C\": C, \"l1_ratio\": l1r}\n",
    "\n",
    "    # Train final model\n",
    "    final_model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lr\", LogisticRegression(\n",
    "            max_iter=5000,\n",
    "            solver=\"saga\",\n",
    "            **best_params,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    final_model.fit(X_tr, y_tr)\n",
    "\n",
    "    joblib.dump(final_model, models_dir / f\"lr_fold{fold}{postfix}.joblib\")\n",
    "    with open(models_dir / f\"lr_fold{fold}{postfix}_params.json\", \"w\") as f:\n",
    "        json.dump(best_params, f)\n",
    "\n",
    "    # Predict\n",
    "    train_probs = final_model.predict_proba(X_tr)[:, 1]\n",
    "    test_probs  = final_model.predict_proba(X_te)[:, 1]\n",
    "\n",
    "    all_rows.append({\"set\":\"train\",\"fold\":fold,**make_metrics(y_tr, train_probs)})\n",
    "    all_rows.append({\"set\":\"test\",\"fold\":fold,**make_metrics(y_te, test_probs)})\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"index\": test_idx,\n",
    "        \"fold\": fold,\n",
    "        \"y_true\": y_te,\n",
    "        \"y_score\": test_probs\n",
    "    }).to_csv(preds_dir / f\"lr_test_fold{fold}{postfix}.csv\", index=False)\n",
    "\n",
    "    oof_preds[test_idx] = test_probs\n",
    "    oof_true[test_idx]  = y_te\n",
    "\n",
    "pd.DataFrame(all_rows).to_csv(metrics_dir / f\"lr_metrics_all_folds{postfix}.csv\", index=False)\n",
    "pd.DataFrame({\"index\":np.arange(len(y)),\"y_true\":oof_true,\"y_score\":oof_preds}).to_csv(\n",
    "    preds_dir / f\"lr_oof{postfix}.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2debb1-27fa-4b14-906e-70b214d2254e",
   "metadata": {},
   "source": [
    "# PHASE C /A/Classifers - XGBoost Classifer with nested CV\n",
    "\n",
    "- Expects X, y, feature_names in memory\n",
    "- Uses saved train/test indices: saved_folds/fold{n}_idx.npz\n",
    "- Performs 3-fold inner CV for hyperparameter tuning\n",
    "- Saves models, metrics (train/val/test), and OOF predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41312ef-770b-42a6-a6a2-f1d55d224b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- Config ---\n",
    "base        = Path(\"saved_folds\").parent\n",
    "folds_dir   = base / \"saved_folds\"\n",
    "models_dir  = base / f\"saved_models_xgb{postfix}\"\n",
    "preds_dir   = base / f\"preds_xgb{postfix}\"\n",
    "metrics_dir = base / f\"metrics_xgb{postfix}\"\n",
    "inner_val_dir = metrics_dir / \"inner_val\"\n",
    "\n",
    "for d in [models_dir, preds_dir, metrics_dir, inner_val_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "n_splits_outer = 5\n",
    "n_splits_inner = 3\n",
    "\n",
    "grid_xgb = {\n",
    "    \"learning_rate\": [0.03],\n",
    "    \"max_depth\": [2,3],\n",
    "    \"subsample\": [0.8,1.0],\n",
    "    \"colsample_bytree\": [1.0]\n",
    "}\n",
    "\n",
    "def make_metrics(y_true, p_prob, thr=0.5):\n",
    "    y_hat = (p_prob >= thr).astype(int)\n",
    "    return {\n",
    "        \"auc\":  roc_auc_score(y_true, p_prob),\n",
    "        \"acc\":  accuracy_score(y_true, y_hat),\n",
    "        \"prec\": precision_score(y_true, y_hat, zero_division=0),\n",
    "        \"rec\":  recall_score(y_true, y_hat, zero_division=0),\n",
    "        \"f1\":   f1_score(y_true, y_hat, zero_division=0),\n",
    "    }\n",
    "\n",
    "# --- Nested CV ---\n",
    "all_rows = []\n",
    "oof_preds = np.zeros(len(y))\n",
    "oof_true  = np.zeros(len(y), dtype=int)\n",
    "\n",
    "for fold in range(1, n_splits_outer+1):\n",
    "    idx = np.load(folds_dir / f\"fold{fold}_idx.npz\", allow_pickle=True)\n",
    "    train_idx, test_idx = idx[\"train_idx\"], idx[\"test_idx\"]\n",
    "\n",
    "    best_auc, best_params = -np.inf, None\n",
    "\n",
    "    inner_cv = StratifiedKFold(n_splits=n_splits_inner, shuffle=True, random_state=42)\n",
    "    for lr, md, ss, cs in itertools.product(\n",
    "        grid_xgb[\"learning_rate\"],\n",
    "        grid_xgb[\"max_depth\"],\n",
    "        grid_xgb[\"subsample\"],\n",
    "        grid_xgb[\"colsample_bytree\"]\n",
    "    ):\n",
    "        aucs_inner = []\n",
    "        val_records = []\n",
    "\n",
    "        for i, (it, iv) in enumerate(inner_cv.split(X[train_idx],y[train_idx]),start=1):\n",
    "            sub_idx = train_idx[it]\n",
    "            val_idx = train_idx[iv]\n",
    "\n",
    "            model = XGBClassifier(\n",
    "                objective=\"binary:logistic\",\n",
    "                eval_metric=\"auc\",\n",
    "                tree_method=\"hist\",\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                n_estimators=2000,\n",
    "                learning_rate=lr,\n",
    "                max_depth=md,\n",
    "                subsample=ss,\n",
    "                colsample_bytree=cs\n",
    "            )\n",
    "            model.fit(X[sub_idx], y[sub_idx], verbose=False)\n",
    "            val_probs = model.predict_proba(X[val_idx])[:,1]\n",
    "\n",
    "            m = make_metrics(y[val_idx], val_probs)\n",
    "            m.update({\"outer_fold\":fold,\"inner_fold\":i,\"lr\":lr,\"md\":md,\"ss\":ss,\"cs\":cs})\n",
    "            val_records.append(m)\n",
    "            aucs_inner.append(m[\"auc\"])\n",
    "\n",
    "        pd.DataFrame(val_records).to_csv(\n",
    "            inner_val_dir / f\"xgb_inner_val_fold{fold}_lr{lr}_md{md}_ss{ss}_cs{cs}{postfix}.csv\",\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        mean_auc = np.mean(aucs_inner)\n",
    "        if mean_auc > best_auc:\n",
    "            best_auc = mean_auc\n",
    "            best_params = {\"learning_rate\": lr, \"max_depth\": md, \"subsample\": ss, \"colsample_bytree\": cs}\n",
    "\n",
    "    # Train final XGB model\n",
    "    final_model = XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        n_estimators=2000,\n",
    "        **best_params\n",
    "    )\n",
    "    final_model.fit(X[train_idx], y[train_idx], verbose=False)\n",
    "\n",
    "    joblib.dump(final_model, models_dir / f\"xgb_fold{fold}{postfix}.joblib\")\n",
    "    with open(models_dir / f\"xgb_fold{fold}{postfix}_params.json\", \"w\") as f:\n",
    "        json.dump(best_params, f)\n",
    "\n",
    "    # Predictions\n",
    "    train_probs = final_model.predict_proba(X[train_idx])[:,1]\n",
    "    test_probs  = final_model.predict_proba(X[test_idx])[:,1]\n",
    "\n",
    "    all_rows.append({\"set\":\"train\",\"fold\":fold,**make_metrics(y[train_idx],train_probs)})\n",
    "    all_rows.append({\"set\":\"test\",\"fold\":fold,**make_metrics(y[test_idx], test_probs)})\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"index\": test_idx,\n",
    "        \"fold\": fold,\n",
    "        \"y_true\": y[test_idx],\n",
    "        \"y_score\": test_probs\n",
    "    }).to_csv(preds_dir / f\"xgb_test_fold{fold}{postfix}.csv\", index=False)\n",
    "\n",
    "    oof_preds[test_idx] = test_probs\n",
    "    oof_true[test_idx]  = y[test_idx]\n",
    "\n",
    "# Aggregate\n",
    "pd.DataFrame(all_rows).to_csv(metrics_dir / f\"xgb_metrics_all_folds{postfix}.csv\",index=False)\n",
    "pd.DataFrame({\"index\":np.arange(len(y)),\"y_true\":oof_true,\"y_score\":oof_preds}).to_csv(\n",
    "    preds_dir / f\"xgb_oof{postfix}.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6412396-1b86-4611-beb7-846a13367fa7",
   "metadata": {},
   "source": [
    "# PHASE D /A/Classifers -SHAP Interpretation of XGBoost Models\n",
    "\n",
    "This notebook computes SHAP (SHapley Additive exPlanations) values for XGBoost models that were previously trained using fixed outer cross-validation splits.  \n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Compute SHAP values **only on held-out test sets** for each outer CV fold.\n",
    "- Quantify:\n",
    "  - Mean absolute SHAP importance per feature.\n",
    "  - Mean signed SHAP contribution per feature.\n",
    "  - Correlation between feature value and SHAP attribution.\n",
    "- Aggregate SHAP statistics across folds and calculate 95% confidence intervals.\n",
    "- Export:\n",
    "  - Per-fold SHAP importance tables.\n",
    "  - A final aggregated SHAP importance table.\n",
    "\n",
    "### Inputs (already in memory)\n",
    "\n",
    "- `X`: numpy array `(n_samples Ã— n_features)`\n",
    "- `y`: numpy array `(n_samples,)`\n",
    "- `feature_names`: list/array of feature names\n",
    "- `postfix`: experiment label (e.g. `_2`)\n",
    "\n",
    "### Required files on disk\n",
    "\n",
    "- Saved models:  \n",
    "  `saved_models{postfix}/xgb_fold{fold}{postfix}.joblib`\n",
    "- Fixed CV indices:  \n",
    "  `saved_folds/fold{fold}_idx.npz`\n",
    "\n",
    "### Outputs\n",
    "\n",
    "- Per-fold SHAP tables:  \n",
    "  `shap_xgb{postfix}/xgb_shap_importance_fold{fold}{postfix}.csv`\n",
    "- Aggregated SHAP summary:  \n",
    "  `shap_xgb{postfix}/xgb_shap_importance_agg{postfix}.csv`\n",
    "\n",
    "This approach ensures **no data leakage**, since SHAP values are computed only on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249570b-c793-4cce-9ffb-5c073991de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, joblib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Project root (portable, no hard-coded paths)\n",
    "# ------------------------------------------------------\n",
    "BASE_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "\n",
    "folds_dir  = BASE_DIR / \"saved_folds\"\n",
    "models_dir = BASE_DIR / f\"saved_models{postfix}\"\n",
    "shap_dir   = BASE_DIR / f\"shap_xgb{postfix}\"\n",
    "shap_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "N_FOLDS = 5\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Sanity checks\n",
    "# ------------------------------------------------------\n",
    "assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray)\n",
    "feature_names = np.asarray(feature_names, dtype=object)\n",
    "assert X.shape[1] == len(feature_names)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------------------------------\n",
    "def ci95(vals):\n",
    "    vals = np.asarray(vals, float)\n",
    "    n = np.isfinite(vals).sum()\n",
    "    if n <= 1:\n",
    "        return np.nanmean(vals), np.nan, np.nan, np.nan\n",
    "    m, s = np.nanmean(vals), np.nanstd(vals, ddof=1)\n",
    "    h = t.ppf(0.975, n-1) * s / np.sqrt(n)\n",
    "    return m, s, m-h, m+h\n",
    "\n",
    "def safe_corr(x, y):\n",
    "    if np.allclose(x, x[0]) or np.allclose(y, y[0]):\n",
    "        return np.nan\n",
    "    return np.corrcoef(x, y)[0,1]\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Main SHAP loop\n",
    "# ------------------------------------------------------\n",
    "abs_map, sgn_map, cor_map = {}, {}, {}\n",
    "\n",
    "for fold in range(1, N_FOLDS+1):\n",
    "    print(f\"Processing fold {fold}\")\n",
    "    idx = np.load(folds_dir / f\"fold{fold}_idx.npz\", allow_pickle=True)\n",
    "    te_idx = idx[\"test_idx\"]\n",
    "\n",
    "    model = joblib.load(models_dir / f\"xgb_fold{fold}{postfix}.joblib\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "\n",
    "    shap_vals = explainer.shap_values(X[te_idx])\n",
    "    if isinstance(shap_vals, list):\n",
    "        shap_vals = shap_vals[1]\n",
    "\n",
    "    abs_m = np.abs(shap_vals).mean(0)\n",
    "    sgn_m = shap_vals.mean(0)\n",
    "    corrs = [safe_corr(X[te_idx,i], shap_vals[:,i]) for i in range(X.shape[1])]\n",
    "\n",
    "    df_fold = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"mean_abs_shap\": abs_m,\n",
    "        \"mean_signed_shap\": sgn_m,\n",
    "        \"corr_feat_shap\": corrs\n",
    "    }).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "\n",
    "    df_fold.to_csv(shap_dir / f\"xgb_shap_importance_fold{fold}{postfix}.csv\", index=False)\n",
    "\n",
    "    for f,a,s,c in zip(feature_names, abs_m, sgn_m, corrs):\n",
    "        abs_map.setdefault(f,[]).append(a)\n",
    "        sgn_map.setdefault(f,[]).append(s)\n",
    "        cor_map.setdefault(f,[]).append(c)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Aggregate across folds\n",
    "# ------------------------------------------------------\n",
    "rows = []\n",
    "for f in feature_names:\n",
    "    ma,sa,la,ha = ci95(abs_map[f])\n",
    "    ms,ss,ls,hs = ci95(sgn_map[f])\n",
    "    mc,sc,lc,hc = ci95(cor_map[f])\n",
    "\n",
    "    direction = \"uncertain\"\n",
    "    if lc > 0: direction=\"positive\"\n",
    "    elif hc < 0: direction=\"negative\"\n",
    "\n",
    "    rows.append([f,ma,sa,la,ha,ms,ss,ls,hs,mc,sc,lc,hc,direction])\n",
    "\n",
    "agg = pd.DataFrame(rows, columns=[\n",
    "    \"feature\",\"mean_abs\",\"sd_abs\",\"ci95_low_abs\",\"ci95_high_abs\",\n",
    "    \"mean_signed\",\"sd_signed\",\"ci95_low_signed\",\"ci95_high_signed\",\n",
    "    \"mean_corr\",\"sd_corr\",\"ci95_low_corr\",\"ci95_high_corr\",\"direction\"\n",
    "]).sort_values(\"mean_abs\", ascending=False)\n",
    "\n",
    "agg.to_csv(shap_dir / f\"xgb_shap_importance_agg{postfix}.csv\", index=False)\n",
    "\n",
    "print(\"SHAP aggregation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c697de0-dc96-46fe-8b23-8aff6c23ae9e",
   "metadata": {},
   "source": [
    "# PHASE C /B/Regressor Nested Cross-Validation Workflow for Quantile Regression\n",
    "\n",
    "This code implements a **nested cross-validation pipeline** to train and evaluate **Quantile Regression models** for predicting Length of Stay (LOS) with predictive intervals.\n",
    "\n",
    "### What the Script Does\n",
    "\n",
    "1. **Imports and Setup**  \n",
    "   Loads required packages (e.g., NumPy, pandas, scikit-learn) and defines helper functions to compute metrics such as RMSE, Pearson and Spearman correlations, and basic regression metrics.\n",
    "\n",
    "2. **Configuration**  \n",
    "   - Sets random seed for reproducibility.\n",
    "   - Defines paths for data, saved folds, models, metrics, and predictions.\n",
    "   - Creates directories if they do not already exist.\n",
    "   - Defines a hyperparameter grid for the Quantile Regressor.\n",
    "   - Defines a list of quantiles (`0.05`, `0.50`, `0.95`) to train prediction intervals.\n",
    "\n",
    "3. **Nested Cross-Validation Loop**\n",
    "   - Loads precomputed outer CV fold indices (train/test splits).\n",
    "   - For each outer fold:\n",
    "     - Splits data into train, validation, and test sets.\n",
    "     - Applies a log transform (`log1p`) to the target variable for stability.\n",
    "     - Runs an **inner 3-fold CV loop** to select the best hyperparameters (based on median MAE in log scale).\n",
    "     - Trains Quantile Regression models for each quantile (lower, median, upper) using the best hyperparameters found.\n",
    "     - Evaluates models on the outer validation and test sets.\n",
    "     - Computes performance metrics on both log and original scales (e.g., MAE, RÂ², Spearman correlation).\n",
    "     - Saves predictions, per-fold metrics, and models.\n",
    "\n",
    "4. **Performance and Prediction Output**\n",
    "   - Predicts for each quantile to form prediction intervals for LOS.\n",
    "   - Saves test set predictions (median and interval bounds).\n",
    "   - Saves nested CV metrics per fold (train/validation/test).\n",
    "   - Computes and saves out-of-fold (OOF) predictions.\n",
    "   - Outputs summary results and ends the script with a completion message.\n",
    "\n",
    "### Structure\n",
    "\n",
    "- **Outer Loop:** Iterates over saved folds.\n",
    "- **Inner Loop:** Hyperparameter tuning via KFold.\n",
    "- **Models Trained:**  \n",
    "  Quantile Regression models for each quantile:  \n",
    "  - `0.05` â†’ Lower bound  \n",
    "  - `0.50` â†’ Median estimate  \n",
    "  - `0.95` â†’ Upper bound\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "- **Original scale:** MAE, RMSE, RÂ², Spearman correlation  \n",
    "- **Interval statistics:** Coverage (10â€“90%) and mean interval width  \n",
    "- **Log scale:** MAE, RÂ² and correlation for log-transformed target\n",
    "\n",
    "The code below carries out these steps and saves outputs into organized folders for models, metrics, and predictions, ready for post-hoc analysis and figure generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49d00c-0e9a-4a77-9f1a-c197264bdd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nested Quantile Regression with Generic Paths\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import itertools\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ================= Helper Functions =================\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def pearson_r(y_true, y_pred):\n",
    "    return pearsonr(y_true, y_pred)[0] if len(y_true) > 2 else np.nan\n",
    "\n",
    "def spearman_r(y_true, y_pred):\n",
    "    return spearmanr(y_true, y_pred)[0] if len(y_true) > 2 else np.nan\n",
    "\n",
    "def metrics_basic(y_true, y_pred):\n",
    "    return {\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"rmse\": rmse(y_true, y_pred),\n",
    "        \"r2\": r2_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "# ================= Config =================\n",
    "\n",
    "RNG = 42\n",
    "\n",
    "# Use current working directory as base for outputs\n",
    "base = Path.cwd()\n",
    "\n",
    "folds_dir    = base / \"saved_folds\"\n",
    "models_dir   = base / f\"saved_models{postfix}\"\n",
    "metrics_dir  = base / f\"metrics_qr{postfix}\"\n",
    "preds_dir    = base / f\"preds_qr{postfix}\"\n",
    "inner_val_dir = metrics_dir / \"inner_val\"\n",
    "\n",
    "# Create directories if needed\n",
    "for d in (models_dir, metrics_dir, preds_dir, inner_val_dir):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparameter grid for Quantile Regression\n",
    "GRID = {\n",
    "    \"alpha\": [1e-4, 1e-3, 1e-2, 0.05],\n",
    "    \"solver\": [\"highs\"],\n",
    "}\n",
    "\n",
    "# Quantiles for building prediction intervals\n",
    "QUANTILES = [0.05, 0.50, 0.95]\n",
    "\n",
    "# Containers for aggregated results\n",
    "all_rows = []\n",
    "oof_preds = np.zeros(len(y))\n",
    "oof_true  = np.zeros(len(y))\n",
    "\n",
    "# Find saved fold index files\n",
    "fold_files = sorted(\n",
    "    glob.glob(str(folds_dir / \"fold*_idx.npz\")),\n",
    "    key=lambda p: int(re.search(r\"fold(\\d+)_idx\\.npz\", os.path.basename(p)).group(1))\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Starting nested cross-validation for Quantile Regression ...\")\n",
    "\n",
    "# ================= Outer CV Loop =================\n",
    "\n",
    "for fold_no, npz_path in enumerate(fold_files, start=1):\n",
    "    print(f\"\\n=== Outer Fold {fold_no}/{len(fold_files)} ===\")\n",
    "\n",
    "    idx = np.load(npz_path, allow_pickle=True)\n",
    "    tr_idx, te_idx = idx[\"train_idx\"], idx[\"test_idx\"]\n",
    "    tr_sub_idx, val_idx = idx[\"train_sub_idx\"], idx[\"val_idx\"]\n",
    "\n",
    "    # Slice data\n",
    "    X_tr, y_tr = X[tr_idx], y[tr_idx].astype(float)\n",
    "    X_te, y_te = X[te_idx], y[te_idx].astype(float)\n",
    "    X_tr_sub, y_tr_sub = X[tr_sub_idx], y[tr_sub_idx].astype(float)\n",
    "    X_val, y_val = X[val_idx], y[val_idx].astype(float)\n",
    "\n",
    "    # Log-transform targets\n",
    "    z_tr = np.log1p(y_tr)\n",
    "    z_tr_sub = np.log1p(y_tr_sub)\n",
    "    z_val = np.log1p(y_val)\n",
    "\n",
    "    # ---------- Inner CV for Hyperparameter Tuning ----------\n",
    "\n",
    "    inner_cv = KFold(n_splits=3, shuffle=True, random_state=RNG)\n",
    "    best_val_mae_log = np.inf\n",
    "    best_params = {}\n",
    "    val_log = []\n",
    "\n",
    "    val_pred_log_all = []\n",
    "    val_true_log_all = []\n",
    "\n",
    "    for alpha, solver in itertools.product(GRID[\"alpha\"], GRID[\"solver\"]):\n",
    "        fold_maes = []\n",
    "        for i, (inner_tr_i, inner_val_i) in enumerate(inner_cv.split(X_tr_sub), start=1):\n",
    "            X_it, X_iv = X_tr_sub[inner_tr_i], X_tr_sub[inner_val_i]\n",
    "            z_it, z_iv = z_tr_sub[inner_tr_i], z_tr_sub[inner_val_i]\n",
    "\n",
    "            preds_val = {}\n",
    "            for q in QUANTILES:\n",
    "                qr = QuantileRegressor(quantile=q, alpha=alpha, solver=solver)\n",
    "                qr.fit(X_it, z_it)\n",
    "                preds_val[q] = qr.predict(X_iv)\n",
    "\n",
    "            mae_val_log = mean_absolute_error(z_iv, preds_val[0.50])\n",
    "            fold_maes.append(mae_val_log)\n",
    "\n",
    "            val_log.append({\n",
    "                \"fold\": fold_no,\n",
    "                \"inner_fold\": i,\n",
    "                \"alpha\": alpha,\n",
    "                \"solver\": solver,\n",
    "                \"val_mae_log\": mae_val_log\n",
    "            })\n",
    "\n",
    "            val_pred_log_all.extend(preds_val[0.50])\n",
    "            val_true_log_all.extend(z_iv)\n",
    "\n",
    "        mean_fold_mae_log = np.mean(fold_maes)\n",
    "        if mean_fold_mae_log < best_val_mae_log:\n",
    "            best_val_mae_log = mean_fold_mae_log\n",
    "            best_params = {\"alpha\": alpha, \"solver\": solver}\n",
    "\n",
    "    # Save inner validation results\n",
    "    pd.DataFrame(val_log).to_csv(inner_val_dir / f\"qr_inner_val_fold{fold_no}{postfix}.csv\", index=False)\n",
    "\n",
    "    print(f\"Best params for fold {fold_no}: {best_params} | Inner MAE (log) = {best_val_mae_log:.4f}\")\n",
    "\n",
    "    # ---------- Fit Best Models and Save ----------\n",
    "\n",
    "    best_models = {}\n",
    "    for q in QUANTILES:\n",
    "        qr = QuantileRegressor(\n",
    "            quantile=q,\n",
    "            alpha=best_params[\"alpha\"],\n",
    "            solver=best_params[\"solver\"]\n",
    "        )\n",
    "        qr.fit(X_tr, z_tr)\n",
    "        best_models[q] = qr\n",
    "        joblib.dump(qr, models_dir / f\"qr_fold{fold_no}_q{q:.2f}{postfix}.joblib\")\n",
    "\n",
    "    # ---------- Predictions and Metrics ----------\n",
    "\n",
    "    z_lower = best_models[0.05].predict(X_te)\n",
    "    z_med   = best_models[0.50].predict(X_te)\n",
    "    z_upper = best_models[0.95].predict(X_te)\n",
    "\n",
    "    y_lower = np.expm1(z_lower)\n",
    "    y_med   = np.expm1(z_med)\n",
    "    y_upper = np.expm1(z_upper)\n",
    "\n",
    "    # Train metrics (log + original scale)\n",
    "    z_tr_pred_full = best_models[0.50].predict(X_tr)\n",
    "    train_mae_log = mean_absolute_error(z_tr, z_tr_pred_full)\n",
    "    train_corr_log = spearman_r(z_tr, z_tr_pred_full)\n",
    "\n",
    "    y_tr_pred_full = np.expm1(z_tr_pred_full)\n",
    "    m_train_org = metrics_basic(y_tr, y_tr_pred_full)\n",
    "    train_corr_org = spearman_r(y_tr, y_tr_pred_full)\n",
    "\n",
    "    # Outer validation metrics\n",
    "    val_true_log_arr = np.array(val_true_log_all)\n",
    "    val_pred_log_arr = np.array(val_pred_log_all)\n",
    "\n",
    "    val_mae_log = mean_absolute_error(val_true_log_arr, val_pred_log_arr)\n",
    "    val_corr_log = spearman_r(val_true_log_arr, val_pred_log_arr)\n",
    "\n",
    "    val_true_org = np.expm1(val_true_log_arr)\n",
    "    val_pred_org = np.expm1(val_pred_log_arr)\n",
    "    val_mae_org = mean_absolute_error(val_true_org, val_pred_org)\n",
    "    val_corr_org = spearman_r(val_true_org, val_pred_org)\n",
    "\n",
    "    # Test metrics\n",
    "    test_mae_log = mean_absolute_error(np.log1p(y_te), z_med)\n",
    "    test_corr_log = spearman_r(np.log1p(y_te), z_med)\n",
    "    m_test_org = metrics_basic(y_te, y_med)\n",
    "    test_corr_org = spearman_r(y_te, y_med)\n",
    "\n",
    "    # Interval metrics\n",
    "    cov_10_90 = np.mean((y_te >= y_lower) & (y_te <= y_upper))\n",
    "    mean_width = np.mean(y_upper - y_lower)\n",
    "\n",
    "    # Save test predictions per fold\n",
    "    pd.DataFrame({\n",
    "        \"index\": te_idx,\n",
    "        \"y_true\": y_te,\n",
    "        \"y_pred_median\": y_med,\n",
    "        \"pi_lower_005\": y_lower,\n",
    "        \"pi_upper_095\": y_upper\n",
    "    }).to_csv(preds_dir / f\"qr_test_preds_fold{fold_no}{postfix}.csv\", index=False)\n",
    "\n",
    "    # Append summary row\n",
    "    all_rows.append({\n",
    "        \"fold\": fold_no,\n",
    "        \"train_mae_org\": m_train_org[\"mae\"],\n",
    "        \"val_mae_org\": val_mae_org,\n",
    "        \"test_mae_org\": m_test_org[\"mae\"],\n",
    "        \"train_corr_org\": train_corr_org,\n",
    "        \"val_corr_org\": val_corr_org,\n",
    "        \"test_corr_org\": test_corr_org,\n",
    "        \"test_rmse_org\": m_test_org[\"rmse\"],\n",
    "        \"test_cov_10_90\": cov_10_90,\n",
    "        \"pi_mean_width\": mean_width\n",
    "    })\n",
    "\n",
    "    oof_preds[te_idx] = y_med\n",
    "    oof_true[te_idx]  = y_te\n",
    "\n",
    "# ---------------- Save Aggregated Results ----------------\n",
    "\n",
    "df_all = pd.DataFrame(all_rows).sort_values(\"fold\")\n",
    "df_all.to_csv(metrics_dir / f\"qr_nestedcv_metrics{postfix}.csv\", index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"index\": np.arange(len(y)),\n",
    "    \"y_true\": oof_true,\n",
    "    \"y_pred\": oof_preds\n",
    "}).to_csv(preds_dir / f\"qr_oof_preds{postfix}.csv\", index=False)\n",
    "\n",
    "print(\"\\n>>> Quantile Regression nested CV complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879dbc6-a0b1-4552-b469-a5d1ab0ed395",
   "metadata": {},
   "source": [
    "# PHASE C /B/Regressor Nested Cross-Validation with NGBoost for Probabilistic Regression\n",
    "\n",
    "This script implements a **nested cross-validation (CV) pipeline** for training and evaluating **NGBoost** (Natural Gradient Boosting) models to predict continuous outcomes (e.g., Length of Stay) with **predictive uncertainty**.\n",
    "\n",
    "NGBoost extends traditional gradient boosting by fitting a **full conditional probability distribution** rather than a single point estimate, allowing for *prediction intervals* and uncertainty quantification. [oai_citation:0â€¡Stanford Machine Learning Group](https://stanfordmlgroup.github.io/projects/ngboost/?utm_source=chatgpt.com)\n",
    "\n",
    "---\n",
    "\n",
    "### Overview of the Approach\n",
    "\n",
    "1. **Input and Setup**\n",
    "\n",
    "   - Imports standard Python libraries (NumPy, pandas, SciPy) and ML tools (StratifiedKFold, DecisionTreeRegressor, NGBoost).  \n",
    "   - Defines helper functions for regression metrics including MAE, RMSE, RÂ², Pearson *r*, and Spearman *r*.\n",
    "\n",
    "2. **Configuration**\n",
    "\n",
    "   - Sets a random seed (`RNG = 42`) for reproducibility.\n",
    "   - Defines three target quantiles (`0.025`, `0.50`, `0.975`) to capture 95% prediction intervals.\n",
    "   - Specifies directories for saved CV fold indices, trained models, evaluation metrics, and predictions.\n",
    "   - Defines an inner-loop hyperparameter grid for learning rate, number of estimators, and tree depth.\n",
    "\n",
    "3. **Nested Cross-Validation Loop**\n",
    "\n",
    "   - Iterates over **outer CV folds** using saved indices.\n",
    "   - For each fold:\n",
    "     - Extracts training and test sets from indices.\n",
    "     - Applies a log1p transform to the target to stabilise variance.\n",
    "     - Uses an **inner StratifiedKFold** on the training set to tune hyperparameters based on median absolute error in log space.\n",
    "     - Fits an NGBoost model using a DecisionTreeRegressor as the base learner and the Normal distribution.\n",
    "     - Saves inner CV performance logs.\n",
    "\n",
    "4. **Final Model Training and Evaluation**\n",
    "\n",
    "   - Trains the selected NGBoost model on the full outer training set.\n",
    "   - Computes predictions and predictive distributions on the test set.\n",
    "     - Calculates median predictions and 95% prediction intervals.\n",
    "   - Evaluates performance on training and test data using error, correlation, and interval coverage metrics.\n",
    "   - Saves:\n",
    "     - Per-fold test predictions with prediction interval bounds.\n",
    "     - Out-of-fold (OOF) point predictions.\n",
    "     - Trained NGBoost models.\n",
    "\n",
    "---\n",
    "\n",
    "### Performance and Outputs\n",
    "\n",
    "The script reports:\n",
    "\n",
    "- **Training and test performance** (MAE, RMSE, RÂ², Pearson and Spearman correlation).\n",
    "- **Prediction interval quality** (coverage and average interval width).\n",
    "- **Per-fold and aggregated metrics** saved in structured CSV format.\n",
    "- **Model artefacts** (serialized with `joblib`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a44fea-e62d-4934-8425-263d44bd8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NGBoost Regression with Log1p Target Transform and 95% Prediction Intervals\n",
    "- Uses saved fold indices: fold{n}_idx.npz containing train_sub_idx, val_idx, test_idx\n",
    "- Hyperparameter tuning on validation set (log scale)\n",
    "- Evaluates point metrics and prediction-interval quality on test set\n",
    "- Saves models, per-fold predictions, and CV summary\n",
    "\n",
    "Expected in memory before running:\n",
    "    X : np.ndarray (n_samples, n_features)\n",
    "    y : np.ndarray (n_samples,)\n",
    "\"\"\"\n",
    "\n",
    "import os, re, glob, itertools, joblib, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.distns import Normal\n",
    "\n",
    "# ------------------\n",
    "# Helper functions\n",
    "# ------------------\n",
    "def rmse(y_true, y_pred):\n",
    "    try:\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)\n",
    "    except TypeError:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"rmse\": rmse(y_true, y_pred),\n",
    "        \"r2\": r2_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "# ------------------\n",
    "# Config / Paths\n",
    "# ------------------\n",
    "RNG       = 42\n",
    "QUANTILES = [0.025, 0.50, 0.975]  # 95% PI\n",
    "\n",
    "base        = Path(\"/Users/daneshm/Documents/Kp_KAIMRC\")\n",
    "folds_dir   = base / \"saved_folds\"\n",
    "models_dir  = base / \"widthsaved_models\"\n",
    "metrics_dir = base / \"widthmetrics_ngb_log1p\"\n",
    "preds_dir   = base / \"widthpreds_ngb_log1p\"\n",
    "\n",
    "for d in (models_dir, metrics_dir, preds_dir):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------\n",
    "# Load folds\n",
    "# ------------------\n",
    "fold_files = sorted(\n",
    "    glob.glob(str(folds_dir / \"fold*_idx.npz\")),\n",
    "    key=lambda p: int(re.search(r\"fold(\\d+)_idx\\.npz\", os.path.basename(p)).group(1))\n",
    ")\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "# ===================\n",
    "# Cross-validation\n",
    "# ===================\n",
    "for fold_no, npz_path in enumerate(fold_files, start=1):\n",
    "    print(f\"\\n=== Fold {fold_no}/{len(fold_files)} ===\")\n",
    "\n",
    "    idx = np.load(npz_path, allow_pickle=True)\n",
    "    te_idx     = idx[\"test_idx\"]\n",
    "    tr_sub_idx = idx[\"train_sub_idx\"]\n",
    "    val_idx    = idx[\"val_idx\"]\n",
    "\n",
    "    X_tr_sub, y_tr_sub = X[tr_sub_idx], y[tr_sub_idx].astype(float)\n",
    "    X_val,    y_val    = X[val_idx],    y[val_idx].astype(float)\n",
    "    X_te,     y_te     = X[te_idx],     y[te_idx].astype(float)\n",
    "\n",
    "    z_tr_sub = np.log1p(y_tr_sub)\n",
    "    z_val    = np.log1p(y_val)\n",
    "\n",
    "    # Hyper-parameter grid\n",
    "    grid = {\n",
    "        \"learning_rate\": [0.04],\n",
    "        \"n_estimators\":  [100, 150],\n",
    "        \"max_depth\":     [2, 3],\n",
    "    }\n",
    "\n",
    "    # Validation tuning (log scale)\n",
    "    best_mae, best_model, best_params = np.inf, None, None\n",
    "    for lr, n_est, md in itertools.product(grid[\"learning_rate\"],\n",
    "                                          grid[\"n_estimators\"],\n",
    "                                          grid[\"max_depth\"]):\n",
    "\n",
    "        base_tree = DecisionTreeRegressor(max_depth=md, random_state=RNG)\n",
    "        ngb = NGBRegressor(\n",
    "            Dist=Normal,\n",
    "            Base=base_tree,\n",
    "            learning_rate=lr,\n",
    "            n_estimators=n_est,\n",
    "            natural_gradient=True,\n",
    "            verbose=False,\n",
    "            random_state=RNG,\n",
    "        )\n",
    "\n",
    "        ngb.fit(X_tr_sub, z_tr_sub)\n",
    "        z_val_pred = ngb.predict(X_val)\n",
    "        mae_val_log = mean_absolute_error(z_val, z_val_pred)\n",
    "\n",
    "        if mae_val_log < best_mae:\n",
    "            best_mae = mae_val_log\n",
    "            best_model = ngb\n",
    "            best_params = {\"learning_rate\": lr, \"n_estimators\": n_est, \"max_depth\": md}\n",
    "\n",
    "    print(\"Best params:\", best_params)\n",
    "\n",
    "    # ------------------\n",
    "    # Test predictions\n",
    "    # ------------------\n",
    "    dist_te  = best_model.pred_dist(X_te)\n",
    "    q_lower  = np.expm1(dist_te.ppf(QUANTILES[0]))\n",
    "    q_median = np.expm1(dist_te.ppf(QUANTILES[1]))\n",
    "    q_upper  = np.expm1(dist_te.ppf(QUANTILES[2]))\n",
    "    y_te_pred = q_median\n",
    "\n",
    "    # Point metrics\n",
    "    m_org = metrics(y_te, y_te_pred)\n",
    "\n",
    "    # Interval metrics\n",
    "    cov_95  = np.mean((y_te >= q_lower) & (y_te <= q_upper))\n",
    "    widths  = q_upper - q_lower\n",
    "    width_abs = np.mean(widths)\n",
    "    data_range = y_te.max() - y_te.min()\n",
    "    width_norm = width_abs / data_range if data_range > 0 else np.nan\n",
    "    sd_test = np.std(y_te, ddof=0)\n",
    "    width_sd_ratio = width_abs / sd_test if sd_test > 0 else np.nan\n",
    "\n",
    "    print(f\"Test MAE={m_org['mae']:.4f} | RMSE={m_org['rmse']:.4f} | R2={m_org['r2']:.4f}\")\n",
    "    print(f\"95% PI coverage = {cov_95:.3f}\")\n",
    "    print(f\"Mean PI width = {width_abs:.3f} | Width/DataRange = {width_norm:.3f} | Width/SD = {width_sd_ratio:.3f}\")\n",
    "\n",
    "    # Save predictions\n",
    "    pd.DataFrame({\n",
    "        \"y_true\": y_te,\n",
    "        \"y_pred_median\": y_te_pred,\n",
    "        \"pi_lower_025\": q_lower,\n",
    "        \"pi_upper_975\": q_upper,\n",
    "    }).to_csv(preds_dir / f\"ngb_log1p_test_preds_fold{fold_no}.csv\", index=False)\n",
    "\n",
    "    # Save metrics\n",
    "    all_rows.append({\n",
    "        \"fold\": fold_no,\n",
    "        **best_params,\n",
    "        \"test_mae\": m_org[\"mae\"],\n",
    "        \"test_rmse\": m_org[\"rmse\"],\n",
    "        \"test_r2\": m_org[\"r2\"],\n",
    "        \"pi_95_coverage\": cov_95,\n",
    "        \"pi_mean_width\": width_abs,\n",
    "        \"pi_width_norm\": width_norm,\n",
    "        \"pi_width_sd_ratio\": width_sd_ratio,\n",
    "    })\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(best_model, models_dir / f\"ngb_log1p_fold{fold_no}.joblib\")\n",
    "\n",
    "# ------------------\n",
    "# Save CV summary\n",
    "# ------------------\n",
    "df = pd.DataFrame(all_rows).sort_values(\"fold\")\n",
    "df.to_csv(metrics_dir / \"ngb_log1p_cv_with_intervals.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== CV summary ===\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3a11f-9556-4674-bcf8-827edd0559d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE D /B/Regressor for NGBoost Models\n",
    "\n",
    "This script generates SHAP explanations for NGBoost models trained with your cross-validation pipeline.\n",
    "\n",
    "**Features:**\n",
    "- Works with saved NGBoost models (`ngb_log1p_fold{n}.joblib`)\n",
    "- Uses the original fold indices (`fold{n}_idx.npz`) to slice the same `X` used during training\n",
    "- Explains the *mean prediction on log1p scale* (suitable for ranking features)\n",
    "- Outputs per-fold SHAP summary plots (beeswarm & bar) and CSVs\n",
    "- Aggregates per-feature importances across all folds\n",
    "\n",
    "**Requirements (in memory before running):**\n",
    "- `X`: full feature matrix used during training\n",
    "- `feature_names`: list/array of feature names of length `X.shape[1]`\n",
    "- `postfix`: string matching your training script postfix (e.g., `\"_exp1\"`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
